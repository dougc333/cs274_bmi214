{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenda \n",
    "<img src-\"12032019_01.png\">\n",
    "machine learing supervised w /\n",
    "<img src-\"12032019_02.png\">\n",
    "<img src-\"12032019_03.png\">\n",
    "<img src-\"12032019_04.png\">\n",
    "kNN: review of hw2 ,array of genes, output owas binary. negatives: N goes up runtime gues up. \n",
    "problem: curse of dimensionaltiy when you ahve mroe features. harder to do the learning or convergence w/more features.\n",
    "There are 2 things that increase runtime, N, teh size of the data and teh number of features. A larger number of features\n",
    "lead to another problem, the curse of dimensionality. \n",
    "<img src-\"12032019_05.png\">\n",
    "2) binary logistic regression, Train y to logistic loss funciton. \n",
    "one of 2 equations or lecture. prediction: generating both probabilites and labels. 2 steps for prediction, transform\n",
    "    data, apply logit funciton. logit is nonlinear. The linear step is the transformation, he means multiplying the weights\n",
    "    by the input. The linear part is the features or input multiplied by weights. \n",
    "<img src-\"12032019_06.png\">\n",
    "3) build up the logistic regression into multiple learners. This is s Fully connected NN. 1 layer fully conected network\n",
    "is a universal approximator. dont need features, put in data and the neuron learns the weights. THe activation function\n",
    "should be monotonic smooth. \n",
    "<img src-\"12032019_07.png\">\n",
    "4) how to train the FCNN? We have something called a loss function. SGD: finding drerivteive takes to minimum. \n",
    "    \n",
    "<img src-\"12032019_08.png\">\n",
    "gradient descent: forward propagation and backpropagation. how to train NN. \n",
    "<img src-\"12032019_09.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros: \n",
    "    1) great performance only if have lot of data\n",
    "    2) flexible models\n",
    "    3) no feature extraction, end to end. \n",
    "    \n",
    "cons: \n",
    "    1) many parameters to train. from depth and width. \n",
    "    2) hyperparametres, neural network width, number of neurons per layer, depth: number of layers to begin with. \n",
    "    step size for steepest descent;if you go to far and overshoot minima. Wont converge if steo size is too big, will\n",
    "    oscillate around minima. \n",
    "<img src-\"12032019_10.png\">\n",
    "DL architectures: CNN, alex and dan covered these in their lectures. \n",
    "1) CNN: great for text, signaks, images. 1d data. \n",
    "2) RNN: great for text and signals, sequential memory\n",
    "both CNN and RNN are autocorrelating meaning they look at neighbors. \n",
    "3) Autoencoder for high dimensioanl data. reduceds to PCA. nonlinear dimensionlaity reduction. \n",
    "<img src-\"12032019_10.png\">\n",
    "CNN: small filter which moves across the images both vertically and horixontally. \n",
    "RNN: RNN cell, each cell has an output value;  \n",
    "autoencoder: \n",
    "<img src-\"12032019_10.png\">\n",
    "history: \n",
    "1943: threshold logic. \n",
    "1960: gradient descent\n",
    "1974: efficient GD\n",
    "1989: CNNS used for hand written digits\n",
    "1997: RNNS and LSTMs \n",
    "1999: GPU\n",
    "<img src-\"12032019_10.png\">\n",
    "Deep learning for Genomics: Transcropton factor binding, if you have a gene you want to down regulate.crate a drug\n",
    "    that influences TF to stop. build and find some synthetic TF or one that exisgs, predict if this binds to a gene\n",
    "    or other regions. regulatory proteins that bind DNA. The TFs aretn exact match to DNA, there is some variation\n",
    "    or variants. \n",
    "<img src-\"12032019_10.png\">\n",
    "Use DL CNN to do this. sequence AACGTC which is our motif we care about, most of tehse motifs are <10AA. We can represent\n",
    "this sequence as a one hot encoded matrix. \n",
    "\n",
    "<img src-\"12032019_10.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
